import make_tokenizer from "./utils/lexing"

fn r(regex) = new RegExp(regex)

let tokens =
  [{:regex r("\/\/.*")                  :ignore true}
   {:regex r("^#")                      :keyword :hash}
   {:regex r("^\~")                     :keyword :tilde}
   {:regex r("^if\b")                   :keyword :if}
   {:regex r("^is\b")                   :keyword :is}
   {:regex r("^unless\b")               :keyword :unless}
   {:regex r("^else\b")                 :keyword :else}
   {:regex r("^return\b")               :keyword :return}
   {:regex r("^let\b")                  :keyword :let}
   {:regex r("^protocol\b")             :keyword :protocol}
   {:regex r("^for\b")                  :keyword :for}
   {:regex r("^of\b")                   :keyword :of}
   {:regex r("^impl\b")                 :keyword :impl}
   {:regex r("^define\b")               :keyword :define}
   {:regex r("^yield\b")                :keyword :yield}
   {:regex r("^async\b")                :keyword :async}
   {:regex r("^assert\!")               :keyword :assert!}
   {:regex r("^await\b")                :keyword :await}
   {:regex r("^new\b")                  :keyword :new}
   {:regex r("^\=\>")                   :keyword :arrow}
   {:regex r("^\@")                     :keyword :at}
   {:regex r("^\&\&")                   :keyword :and-and}
   {:regex r("^\|\|")                   :keyword :or-or}
   {:regex r("^\=\=\=")                 :keyword :triple-eq}
   {:regex r("^\!\=\=")                 :keyword :triple-not-eq}
   {:regex r("^\=\=")                   :keyword :double-eq}
   {:regex r("^\!\=")                   :keyword :not-eq}
   {:regex r("^\!")                     :keyword :bang}
   {:regex r("^\=")                     :keyword :eq}
   {:regex r("^fn\b")                   :keyword :fn}
   {:regex r("^\{")                     :keyword :open-b}
   {:regex r("^\}")                     :keyword :close-b}
   {:regex r("^\(")                     :keyword :open-p}
   {:regex r("^\)")                     :keyword :close-p}
   {:regex r("^[\-\+]?\d+n")            :keyword :big-int}
   {:regex r("^[\-\+]?(\d*\.)?\d+")     :keyword :num}
   {:regex r("^\.\.\.")                 :keyword :dot-dot-dot}
   {:regex r("^\.\.")                   :keyword :dot-dot}
   {:regex r("^\.")                     :keyword :dot}
   {:regex r("^\>\=")                   :keyword :gt-eq}
   {:regex r("^\<\=")                   :keyword :lt-eq}
   {:regex r("^\<\/")                   :keyword :jsx-close}
   {:regex r("^\>")                     :keyword :gt}
   {:regex r("^\<")                     :keyword :lt}
   {:regex r("^\+")                     :keyword :plus}
   {:regex r("^\%")                     :keyword :mod}
   {:regex r("^\-")                     :keyword :minus}
   {:regex r("^\*\*")                   :keyword :pow}
   {:regex r("^\*")                     :keyword :times}
   {:regex r("^\&")                     :keyword :single-and}
   {:regex r("^\:\:")                   :keyword :double-colon}
   {:regex r("^\:[a-zA-Z_\?\!\$0-9]+")  :keyword :keyword}
   {:regex r("^\:")                     :keyword :colon}
   {:regex r("^\/")                     :keyword :div}
   {:regex r("^\[")                     :keyword :open-sq}
   {:regex r("^\]")                     :keyword :close-sq}
   {:regex r("(?s)^\"([^\\\"]|\\.)*\"") :keyword :string-lit}
   {:regex r("^[a-zA-Z_\?\!\$0-9]+")    :keyword :id}]

let tokenize = make_tokenizer(tokens)

export default tokenize
